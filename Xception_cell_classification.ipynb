{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "from scipy import ndimage\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import requests\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "import sklearn.cross_validation as crv\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image(filename):\n",
    "    img = cv2.imread(filename, 0)\n",
    "    \n",
    "    #print(img.shape)\n",
    "    \n",
    "    #plt.figure(figsize=(40,30))\n",
    "    #plt.imshow(img)\n",
    "    #plt.gray()\n",
    "    #plt.show()\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def get_group_names(directory_path=None):\n",
    "    if directory_path==None:\n",
    "        pics_list = glob.glob(\"*.tif\")\n",
    "    else:\n",
    "        pics_list = glob.glob(directory_path + \"/\" + \"*.tif\")\n",
    "        \n",
    "    pics_list_new = []\n",
    "    \n",
    "    for pics_name in pics_list:\n",
    "        pics_list_new.append(pics_name[:-5])\n",
    "        groups = list(set(pics_list_new))\n",
    "        groups.sort()\n",
    "        \n",
    "    return groups \n",
    "\n",
    "\n",
    "def merge_rgb(group_name):\n",
    "    \n",
    "    r = read_image(group_name+str(1)+\".tif\")\n",
    "    g = read_image(group_name+str(2)+\".tif\")\n",
    "    b = read_image(group_name+str(3)+\".tif\")\n",
    "        \n",
    "    g = cv2.resize(g, (r.shape[1], r.shape[0]))\n",
    "    b = cv2.resize(g, (r.shape[1], r.shape[0]))\n",
    "        \n",
    "    r = r.reshape(r.shape[0], r.shape[1], 1)\n",
    "    g = g.reshape(g.shape[0], g.shape[1], 1)\n",
    "    b = b.reshape(b.shape[0], b.shape[1], 1)\n",
    "        \n",
    "    merged = np.concatenate([r,g,b],axis=2)\n",
    "    \n",
    "    cv2.imwrite(group_name+\"merged.tif\", merged)\n",
    "\n",
    "\n",
    "def main():\n",
    "    groups = get_group_names()\n",
    "    for group in groups:\n",
    "        merge_rgb(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mkdir condition_1\n",
    "! mkdir condition_2\n",
    "! mkdir condition_3\n",
    "! mkdir control_1\n",
    "! mkdir control_2\n",
    "! mkdir control_3\n",
    "! mv condition*1-merged.tif condition_1\n",
    "! mv condition*2-merged.tif condition_2\n",
    "! mv condition*3-merged.tif condition_3\n",
    "! mv control*1-merged.tif control_1\n",
    "! mv control*2-merged.tif control_2\n",
    "! mv control*3-merged.tif control_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 各種パラメータ\n",
    "\n",
    "filenamewithdate = \"directory_name\"\n",
    "\n",
    "path = \"/path/to/directory/\"\n",
    "\n",
    "directory_path_0 = path+filenamewithdate+\"/control_1/\"\n",
    "directory_path_1 = path+filenamewithdate+\"/control_2/\"\n",
    "directory_path_2 = path+filenamewithdate+\"/control_3/\"\n",
    "directory_path_3 = path+filenamewithdate+\"/condition_1/\"\n",
    "directory_path_4 = path+filenamewithdate+\"/condition_2/\"  \n",
    "directory_path_5 = path+filenamewithdate+\"/condition_3/\"\n",
    "\n",
    "pic = 0             # pre_checkで何枚目の画像を表示するかを指定。デフォルトでは0で設定（1枚目）\n",
    "\n",
    "binimg_thred = 80   # 2値化を行う場合の、ピクセル値を全体の何%の部分で2つに分けるかを設定。\n",
    "                    # 位相差画像で色が黒い部分を検出したい場合はFluoroオプションをFalseとし、\n",
    "                    # binimg_thredを1-10程度で調節する（暗いところを検出する）。\n",
    "                    # 免疫染色画像で明るい部分を検出したい場合はFluoroオプションをTrueとし、\n",
    "                    # binimg_thredを80-99程度で調節する（明るいところを検出するようになる）。\n",
    "\n",
    "n_chan=3\n",
    "                \n",
    "chs = 0             # 2値化の際に使用するチャンネル（0:赤,1:緑,2:青）\n",
    "\n",
    "fluoro = True      # 位相差画像で色が黒い部分を検出したい場合はFluoroオプションをFalseとし、\n",
    "                    # 免疫染色画像で明るい部分を検出したい場合はFluoroオプションをTrueに設定する。\n",
    "    \n",
    "min_area = 1000       # 細胞と認識する最小面積\n",
    "\n",
    "max_area = 99999999\n",
    "\n",
    "scale_v = 75        # 切り抜く歳の画像の高さ/2 (px)\n",
    "\n",
    "scale_h = 75        # 切り抜く際の画像の幅/2 (px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def making_pics_list(directory_path):\n",
    "                \n",
    "    path = directory_path + \"*\"  \n",
    "    filenames = glob.glob(path)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "\n",
    "def cell_crop_single(single_img, binimg_thred = 80., min_area=2000, max_area=4000, scale_v=75, scale_h=75, chs=0, fluoro=True):\n",
    "    \n",
    "    cells = np.empty((0, scale_v*2, scale_h*2, 3))\n",
    "    \n",
    "    img = single_img.astype(np.uint8)\n",
    "    img_chs = cv2.split(img)\n",
    "    img_preprocessed = cv2.GaussianBlur(img_chs[chs],(5,5),0)\n",
    "    if fluoro==False:\n",
    "        binimg = (img_preprocessed < np.percentile(img_preprocessed, binimg_thred))\n",
    "        binimg = binimg.astype(np.uint8)\n",
    "    else:\n",
    "        binimg = (img_preprocessed > np.percentile(img_preprocessed, binimg_thred))\n",
    "        binimg = binimg.astype(np.uint8)\n",
    "\n",
    "    img_, contours, _ = cv2.findContours(binimg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    arr=[]\n",
    "    \n",
    "    start=np.empty((0,2))\n",
    "    start=np.append(start,np.array([[0, 0]]),axis=0)\n",
    "    \n",
    "    for j in contours:\n",
    "        if cv2.contourArea(j)<min_area:\n",
    "            continue\n",
    "        if cv2.contourArea(j)>max_area:\n",
    "            continue\n",
    "        x_=0\n",
    "        y_=0\n",
    "        for k in j:\n",
    "            x_ += k[0][0]\n",
    "            y_ += k[0][1]\n",
    "        arr.append([x_/len(j), y_/len(j)])\n",
    "    arr = np.array(arr)\n",
    "    \n",
    "    \n",
    "    for j in range(len(arr)):\n",
    "    \n",
    "        if (arr[j][1] < scale_v) or (arr[j][1] > img.shape[0]-scale_v) or (arr[j][0] < scale_h) or (arr[j][0] > img.shape[1]-scale_h):\n",
    "            continue \n",
    "        \n",
    "        top = int(arr[j][1])-scale_v\n",
    "        bottom = int(arr[j][1])+scale_v\n",
    "    \n",
    "        left = int(arr[j][0])-scale_h\n",
    "        right = int(arr[j][0])+scale_h\n",
    "    \n",
    "        if left < 0:\n",
    "            left = 0\n",
    "            right = scale_h*2\n",
    "        if right > img.shape[1]:\n",
    "            right = img.shape[1]\n",
    "            left = img.shape[1]-scale_h*2\n",
    "    \n",
    "        if top < 0:\n",
    "            top = 0\n",
    "            bottom = scale_v*2\n",
    "        if bottom > img.shape[0]:\n",
    "            bottom = img.shape[0]\n",
    "            top = img.shape[0]-scale_v*2      \n",
    "                \n",
    "        img_crop = np.array(img[top:bottom,left:right]).reshape(scale_v*2, scale_h*2, 3).astype(np.uint8)\n",
    "        img_chs = cv2.split(img_crop)\n",
    "        img_preprocessed = cv2.GaussianBlur(img_chs[chs],(5,5),0)\n",
    "            \n",
    "        if fluoro==False:\n",
    "            binimg = (img_preprocessed < np.percentile(img_preprocessed, binimg_thred))\n",
    "            binimg = binimg.astype(np.uint8)\n",
    "        else:\n",
    "            binimg = (img_preprocessed > np.percentile(img_preprocessed, binimg_thred))\n",
    "            binimg = binimg.astype(np.uint8)\n",
    "\n",
    "        img_, contours, _ = cv2.findContours(binimg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            \n",
    "        contourArea = []\n",
    "            \n",
    "        for j in contours:\n",
    "            contourArea.append(cv2.contourArea(j))\n",
    "        contourArea_sum = sum(contourArea)\n",
    "        if contourArea_sum<min_area:\n",
    "            continue\n",
    "    \n",
    "        cells = np.append(cells,np.array(img[top:bottom,left:right]).reshape(1,scale_v*2, scale_h*2, 3),axis=0)\n",
    "\n",
    "    print(\"cropped_cell_count:\", cells.shape[0])\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 400))\n",
    "    \n",
    "    cropped_cell_count = cells.shape[0]\n",
    "    \n",
    "    if cropped_cell_count>2000:\n",
    "        cropped_cell_count = 2000\n",
    "\n",
    "    for i in range(cropped_cell_count):\n",
    "        \n",
    "        ax_cell = fig.add_subplot(200, 10, i + 1, xticks=[], yticks=[])\n",
    "        ax_cell.imshow(cells[i].reshape((scale_v*2, scale_h*2, 3)).astype(np.uint8))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return cells    \n",
    "    \n",
    "\n",
    "def cell_crop_from_each_pic(filenames):\n",
    "    \n",
    "    global binimg_thred, min_area, scale_v, scale_h, chs, fluoro\n",
    "    \n",
    "    total_cells = np.empty((0, scale_v*2, scale_h*2, 3))\n",
    "    \n",
    "    for filename in filenames:\n",
    "        print(\"filename:\", filename)\n",
    "        img = scipy.misc.imread(filename)\n",
    "        height, width, chan = img.shape\n",
    "        assert chan == 3\n",
    "        cells = cell_crop_single(img, binimg_thred=binimg_thred, min_area=min_area, max_area=max_area, scale_v=scale_v, scale_h=scale_h, chs=chs, fluoro=fluoro)\n",
    "        total_cells = np.append(total_cells,cells,axis=0)\n",
    "    \n",
    "    print(\"total_cropped_cell_count:\", total_cells.shape[0])    \n",
    "    \n",
    "    return total_cells  \n",
    "\n",
    "\n",
    "def pre_check(X, pic=0, binimg_thred = 80., chs=0, fluoro=True):\n",
    "\n",
    "    img = X[pic].astype(np.uint8)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"##### original picture #####\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    img_chs = cv2.split(img)\n",
    "    img_preprocessed = cv2.GaussianBlur(img_chs[chs],(5,5),0)\n",
    "    if fluoro==False:\n",
    "        binimg = (img_preprocessed < np.percentile(img_preprocessed, binimg_thred))\n",
    "        binimg = binimg.astype(np.uint8)\n",
    "    else:\n",
    "        binimg = (img_preprocessed > np.percentile(img_preprocessed, binimg_thred))\n",
    "        binimg = binimg.astype(np.uint8)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"##### post binarization #####\")\n",
    "    plt.imshow(binimg)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def preprocess_input(x0):\n",
    "    return ((x0/255.)-0.5)*2.\n",
    "    \n",
    "\n",
    "\n",
    "def cell_crop_and_select(directory_path):\n",
    "    \n",
    "    filenames = making_pics_list(directory_path)\n",
    "    cells = cell_crop_from_each_pic(filenames)\n",
    "    \n",
    "    return cells\n",
    "\n",
    "\n",
    "def line_notify(message):\n",
    "    \n",
    "    url = \"https://notify-api.line.me/api/notify\"\n",
    "    token = \"UaZd8ZlZz4i2FidTS8cK6j9J4oqc808CRk7lzHQ84qn\"\n",
    "    headers = {\"Authorization\" : \"Bearer \"+ token}\n",
    "    message = \"\\n\" + message\n",
    "    payload = {\"message\" :  message}\n",
    "\n",
    "    r = requests.post(url ,headers = headers ,params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pics_0 = cell_crop_and_select(directory_path_0)\n",
    "pics_1 = cell_crop_and_select(directory_path_1)\n",
    "pics_2 = cell_crop_and_select(directory_path_2)\n",
    "pics_3 = cell_crop_and_select(directory_path_3)\n",
    "pics_4 = cell_crop_and_select(directory_path_4)\n",
    "pics_5 = cell_crop_and_select(directory_path_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pics_0.shape)\n",
    "print(pics_1.shape)\n",
    "print(pics_2.shape)\n",
    "print(pics_3.shape)\n",
    "print(pics_4.shape)\n",
    "print(pics_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"pics_control_1_\"+filenamewithdate+\".npy\", pics_0)\n",
    "np.save(\"pics_control_2_\"+filenamewithdate+\".npy\", pics_1)\n",
    "np.save(\"pics_control_3_\"+filenamewithdate+\".npy\", pics_2)\n",
    "np.save(\"pics_condition_1_\"+filenamewithdate+\".npy\", pics_3)\n",
    "np.save(\"pics_condition_2_\"+filenamewithdate+\".npy\", pics_4)\n",
    "np.save(\"pics_condition_3_\"+filenamewithdate+\".npy\", pics_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deeplerning用各種パラメータ\n",
    "\n",
    "gpu_count = 4\n",
    "n_chan=3\n",
    "\n",
    "scale_v = 75        # 切り抜く歳の画像の高さ/2 (px)\n",
    "scale_h = 75        # 切り抜く際の画像の幅/2 (px)\n",
    "\n",
    "nb_epoch = 2\n",
    "nb_classes = 2\n",
    "batch_size = 128\n",
    "\n",
    "multi_gpu = False\n",
    "use_fit = True\n",
    "data_augmentation_scale =\"small\"  # full, mid, or small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# データ読み込みとリスト作成\n",
    "\n",
    "filenamewithdate = \"directory_name\"\n",
    "path = \"/path/to/directory/\"\n",
    "\n",
    "pics_control_1 = np.load(path+\"/\"+filenamewithdate+\"/pics_control_1_\"+filenamewithdate+\".npy\")\n",
    "pics_condition_1 = np.load(path+\"/\"+filenamewithdate+\"/pics_condition_1_\"+filenamewithdate+\".npy\")\n",
    "pics_control_2 = np.load(path+\"/\"+filenamewithdate+\"/pics_control_2_\"+filenamewithdate+\".npy\")\n",
    "pics_condition_2 = np.load(path+\"/\"+filenamewithdate+\"/pics_condition_2_\"+filenamewithdate+\".npy\")\n",
    "pics_control_3 = np.load(path+\"/\"+filenamewithdate+\"/pics_control_3_\"+filenamewithdate+\".npy\")\n",
    "pics_condition_3 = np.load(path+\"/\"+filenamewithdate+\"/pics_condition_3_\"+filenamewithdate+\".npy\")\n",
    "\n",
    "print(pics_control_1.shape)\n",
    "print(pics_condition_1.shape)\n",
    "print(pics_control_2.shape)\n",
    "print(pics_condition_2.shape)\n",
    "print(pics_control_3.shape)\n",
    "print(pics_condition_3.shape)\n",
    "\n",
    "\n",
    "control_list = [pics_control_1, pics_control_2, pics_control_3]\n",
    "condition_list = [pics_condition_1, pics_condi_2, pics_ang2_3]\n",
    "et1_list = [pics_et1_1, pics_et1_2, pics_et1_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_setting(condition_1, condition_2):\n",
    "    \n",
    "    if type(condition_1)==list and type(condition_2)==list:\n",
    "    \n",
    "        X_tmp_1 = np.empty((0,scale_v*2,scale_h*2,3))\n",
    "        X_tmp_2 = np.empty((0,scale_v*2,scale_h*2,3))\n",
    "    \n",
    "        for i in range(len(condition_1)):\n",
    "        \n",
    "            X_tmp_1 = np.concatenate((X_tmp_1, condition_1[i]),axis=0)\n",
    "    \n",
    "        for i in range(len(condition_2)):\n",
    "        \n",
    "            X_tmp_2 = np.concatenate((X_tmp_2, condition_2[i]),axis=0)    \n",
    "    \n",
    "        X = np.concatenate((X_tmp_1, X_tmp_2),axis=0)\n",
    "        y = np.concatenate((np.tile(np.array([[0]]),(X_tmp_1.shape[0],1)),\n",
    "                             np.tile(np.array([[1]]),(X_tmp_2.shape[0],1))\n",
    "                            ),axis=0)\n",
    "    \n",
    "        X = preprocess_input(X)\n",
    "\n",
    "        X = X.reshape(X.shape[0], scale_v*2, scale_h*2, n_chan)\n",
    "        Y = np_utils.to_categorical(y, 2)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        X = np.concatenate((condition_1, condition_2),axis=0)\n",
    "        y = np.concatenate((np.tile(np.array([[0]]),(condition_1.shape[0],1)),\n",
    "                             np.tile(np.array([[1]]),(condition_2.shape[0],1))\n",
    "                            ),axis=0)\n",
    "    \n",
    "        X = preprocess_input(X)\n",
    "        X = X.reshape(X.shape[0], scale_v*2, scale_h*2, n_chan)\n",
    "    \n",
    "        Y = np_utils.to_categorical(y, 2)\n",
    "        \n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def data_augmentation(X_train, X_test, Y_train, Y_test, scale=data_augmentation_scale):\n",
    "\n",
    "    if scale==\"full\":\n",
    "    \n",
    "        train_X_00 = X_train[:, 15:(scale_v*2 - 15), 15:(scale_h*2 - 15), :] \n",
    " \n",
    "        train_X_flip = X_train[:, :, ::-1, :] \n",
    " \n",
    "        test_X_00 = X_test[:, 15:(scale_v*2 - 15), 15:(scale_h*2 - 15), :] \n",
    " \n",
    "        train_X_01 = X_train[:, 0:(scale_v*2 - 30), 0:(scale_h*2 - 30), :] \n",
    "        train_X_02 = X_train[:, 30:scale_v*2, 30:scale_h*2, :] \n",
    "        train_X_03 = X_train[:, 30:scale_v*2, 0:(scale_h*2 - 30), :] \n",
    "        train_X_04 = X_train[:, 0:(scale_v*2 - 30), 30:scale_h*2, :] \n",
    " \n",
    "        train_X_05 = train_X_flip[:, 0:(scale_v*2 - 30), 0:(scale_h*2 - 30), :] \n",
    "        train_X_06 = train_X_flip[:, 30:scale_v*2, 30:scale_h*2, :] \n",
    "        train_X_07 = train_X_flip[:, 30:scale_v*2, 0:(scale_h*2 - 30), :] \n",
    "        train_X_08 = train_X_flip[:, 0:(scale_v*2 - 30), 30:scale_h*2, :] \n",
    "        train_X_09 = train_X_flip[:, 15:(scale_v*2 - 15), 15:(scale_h*2 - 15), :]\n",
    "\n",
    "        train_X_rotate = np.empty((0,scale_v*2-30,scale_h*2-30,3))\n",
    "\n",
    "        for i in [train_X_00,train_X_01,train_X_02,train_X_03,train_X_04,train_X_05,train_X_06,train_X_07,train_X_08,train_X_09]:\n",
    "            for j in [90,180,270]:\n",
    "                train_X_rotate = np.append(train_X_rotate, ndimage.rotate(i, j, axes=(1,2), reshape=False, mode='nearest'), axis=0) \n",
    "    \n",
    "        X_train = np.concatenate((train_X_00,train_X_01,train_X_02,train_X_03,train_X_04,train_X_05,train_X_06,train_X_07,train_X_08,train_X_09,train_X_rotate),axis=0)\n",
    "        \n",
    "    elif scale==\"mid\":\n",
    "        \n",
    "        train_X_00 = X_train[:, 15:(scale_v*2 - 15), 15:(scale_h*2 - 15), :] \n",
    " \n",
    "        test_X_00 = X_test[:, 15:(scale_v*2 - 15), 15:(scale_h*2 - 15), :] \n",
    "\n",
    "        train_X_rotate = np.empty((0,scale_v*2-30,scale_h*2-30,3))\n",
    "\n",
    "        for i in [train_X_00]:\n",
    "            for j in [90,180,270]:\n",
    "                train_X_rotate = np.append(train_X_rotate, ndimage.rotate(i, j, axes=(1,2), reshape=False, mode='nearest'), axis=0) \n",
    "    \n",
    "        X_train = np.concatenate((train_X_00,train_X_rotate),axis=0)\n",
    "        \n",
    "    elif scale==\"small\":\n",
    "        \n",
    "        train_X_00 = X_train[:, 15:(scale_v*2 - 15), 15:(scale_h*2 - 15), :] \n",
    " \n",
    "        test_X_00 = X_test[:, 15:(scale_v*2 - 15), 15:(scale_h*2 - 15), :] \n",
    "\n",
    "        X_train = train_X_00\n",
    "    \n",
    "    Y_train = np.tile(Y_train, (int(X_train.shape[0]/train_X_00.shape[0]), 1)) \n",
    "    X_test = test_X_00\n",
    "    \n",
    "    print(\"X_train data count:\", X_train.shape[0])\n",
    "    print(\"X_test data count:\", X_test.shape[0])\n",
    " \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "def build_model_xception(nb_classes=2, multi_gpu=multi_gpu):\n",
    "    \n",
    "    keras.backend.clear_session()  \n",
    "    \n",
    "    if multi_gpu==False:\n",
    "    \n",
    "        base_model = Xception(weights='imagenet', include_top=False)\n",
    "\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "        model_ = Model(inputs=base_model.input, outputs=predictions)\n",
    "        model = model_\n",
    "    \n",
    "    if multi_gpu==True:\n",
    "    \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            base_model = Xception(weights='imagenet', include_top=False)\n",
    "    \n",
    "            x = base_model.output\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dense(1024, activation='relu')(x)\n",
    "            predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "            model_ = Model(inputs=base_model.input, outputs=predictions)\n",
    "        \n",
    "        model = multi_gpu_model(model_, gpus=gpu_count)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    \n",
    "    plt.plot(history.history['acc'],\"o-\",label=\"accuracy\")\n",
    "    plt.plot(history.history['val_acc'],\"o-\",label=\"val_acc\")\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.ylim([0,1])\n",
    "    \n",
    "    plt.savefig(datetime.now().strftime('%Y%m%d%H%M%S')+\"acc.png\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'],\"o-\",label=\"loss\",)\n",
    "    plt.plot(history.history['val_loss'],\"o-\",label=\"val_loss\")\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.savefig(datetime.now().strftime('%Y%m%d%H%M%S')+\"loss.png\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    df = pd.DataFrame({ 'accuracy' : history.history['acc'],\n",
    "                        'val_acc' : history.history['val_acc'],\n",
    "                        'loss' : history.history['loss'],\n",
    "                        'val_loss' : history.history['val_loss']})\n",
    "    \n",
    "    df.to_csv(datetime.now().strftime('%Y%m%d%H%M%S')+\".csv\")\n",
    "        \n",
    "\n",
    "\n",
    "def do_xception(X_train, X_test, Y_train, Y_test, multi_gpu=multi_gpu, use_fit=use_fit):\n",
    "    \n",
    "    global batch_size, gpu_count, nb_epoch, model, history_x\n",
    "    \n",
    "    if multi_gpu==False and use_fit==False:\n",
    "    \n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            rotation_range=45,\n",
    "            width_shift_range=0.25,\n",
    "            height_shift_range=0.25,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    "            fill_mode='nearest')\n",
    "    \n",
    "        history_x = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size, shuffle=True),\n",
    "        steps_per_epoch=X_train.shape[0]//batch_size,\n",
    "        epochs=nb_epoch,\n",
    "        validation_data=datagen.flow(X_test, Y_test, batch_size=batch_size),\n",
    "        validation_steps=X_test.shape[0]//batch_size)\n",
    "    \n",
    "    elif multi_gpu==True and use_fit==False:\n",
    "        \n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            rotation_range=0,\n",
    "            width_shift_range=0,\n",
    "            height_shift_range=0,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    "            fill_mode='nearest')\n",
    "    \n",
    "        history_x = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size*gpu_count, shuffle=True),\n",
    "        steps_per_epoch=X_train.shape[0]//(batch_size*gpu_count),\n",
    "        epochs=nb_epoch,\n",
    "        validation_data=datagen.flow(X_test, Y_test, batch_size=batch_size*gpu_count),\n",
    "        validation_steps=X_test.shape[0]//(batch_size*gpu_count),\n",
    "        workers=8,\n",
    "        max_queue_size=16,\n",
    "        use_multiprocessing=True)\n",
    "    \n",
    "    elif multi_gpu==True and use_fit==True:\n",
    "        \n",
    "        history_x = model.fit(x=X_train, y=Y_train, batch_size=(batch_size*gpu_count), shuffle=True,\n",
    "        epochs=nb_epoch,\n",
    "        validation_data=(X_test, Y_test))\n",
    "        \n",
    "    elif multi_gpu==False and use_fit==True:\n",
    "        \n",
    "        history_x = model.fit(x=X_train, y=Y_train, batch_size=batch_size, shuffle=True,\n",
    "        epochs=nb_epoch,\n",
    "        validation_data=(X_test, Y_test))\n",
    "\n",
    "\n",
    "def binary_xception(condition_1, condition_2, test_1, test_2, multi_gpu=multi_gpu, use_fit=use_fit, scale=data_augmentation_scale):\n",
    "    \n",
    "    global nb_classes, batch_size, gpu_count, model, history_x\n",
    "    \n",
    "    X_train, Y_train = data_setting(condition_1, condition_2)\n",
    "    X_test, Y_test = data_setting(test_1, test_2)\n",
    "    X_train, X_test, Y_train, Y_test = data_augmentation(X_train, X_test, Y_train, Y_test, scale=scale)\n",
    "    \n",
    "    model = build_model_xception(nb_classes=nb_classes, multi_gpu=multi_gpu)\n",
    "    \n",
    "    do_xception(X_train, X_test, Y_train, Y_test, multi_gpu=multi_gpu, use_fit=use_fit)\n",
    "    \n",
    "    Y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    print('final accuracy:',f1_score(np.argmax(Y_test,1), y_pred, average='macro'))\n",
    "    \n",
    "    plot_history(history_x)  \n",
    "\n",
    "\n",
    "def cross_validation(control_list, subject_list, multi_gpu=multi_gpu, use_fit=use_fit, scale=data_augmentation_scale):\n",
    "    \n",
    "    global nb_classes, batch_size, gpu_count, model, history_x\n",
    "    \n",
    "    for i in range(len(control_list)):\n",
    "        \n",
    "        control_list_tmp = deepcopy(control_list)\n",
    "        subject_list_tmp = deepcopy(subject_list)\n",
    "        \n",
    "        control_target = control_list_tmp[i]\n",
    "        subject_target = subject_list_tmp[i]\n",
    "        control_list_tmp.pop(i)\n",
    "        subject_list_tmp.pop(i)\n",
    "        \n",
    "        print(\"cross validation:\", i)\n",
    "        \n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        \n",
    "        binary_xception(control_list_tmp, subject_list_tmp, control_target, subject_target, multi_gpu=multi_gpu, use_fit=use_fit, scale=scale)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross_validation(control_list, ang2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
